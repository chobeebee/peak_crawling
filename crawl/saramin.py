from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import requests
import re
import json

# ê¸°ì—…ëª…ì—ì„œ (ì£¼), (ì£¼ì‹íšŒì‚¬) ì ‘ë‘ì‚¬/ì ‘ë¯¸ì‚¬, ê³µë°± ì œê±°
def filtering_company_name(name):
    """
    íšŒì‚¬ ì´ë¦„ì—ì„œ (ì£¼), (ì£¼ì‹íšŒì‚¬) ë“± ì ‘ë‘ì‚¬/ì ‘ë¯¸ì‚¬ ë° ê³µë°± ì œê±°
    """
    print(f"=== filtering_company_name í•¨ìˆ˜ ì‹¤í–‰ ===")
    removal_inc = re.sub(r'\(ì£¼\)', '', name, flags=re.IGNORECASE)
    removal_inc = re.sub(r'\(ì£¼ì‹íšŒì‚¬\)', '', removal_inc, flags=re.IGNORECASE)

    removal_inc = re.sub(r'\s+', ' ', removal_inc).strip()

    print(f"filtering_company_name í•¨ìˆ˜ ê²°ê³¼: {removal_inc}")
    return removal_inc

# ê¸°ì—…ëª… ê²€ìƒ‰, ê²€ìƒ‰ëœ ê¸°ì—…ëª… ë¹„êµ
def compare_company_name(searching_keyword, searched_company):
    """
    ë‘ íšŒì‚¬ ì´ë¦„ì„ (ì£¼), (ì£¼ì‹íšŒì‚¬) ë“±ì˜ ë¬¸ìì—´ì„ ì œì™¸í•˜ê³  ë¹„êµ
    """
    print(f"=== compare_company_name í•¨ìˆ˜ ì‹¤í–‰ ===")
    filtered_company_name = filtering_company_name(searched_company)

    result = searching_keyword == filtered_company_name
    print(f"searching_keyword == filtered_company_name ê²°ê³¼={result}")
    return result


# ì¬ë¬´í˜„í™© íƒ­ ì´ë™
def get_financial_info_after_button(driver, target_button_text, wait_time=10):
    """
    ì¬ë¬´ì •ë³´ ë²„íŠ¼ì„ í´ë¦­í•˜ê³  ì¬ë¬´í˜„í™© ë°ì´í„° ìˆ˜ì§‘í•¨. 
    """
    print(f"=== get_financial_info_after_button í•¨ìˆ˜ ì‹¤í–‰ ===")
    try:
        # 1. ì¢Œì¸¡ ëª¨ë“  ë©”ë‰´ ë²„íŠ¼ ì°¾ê¸°
        wait = WebDriverWait(driver, wait_time)
        buttons = wait.until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, "ul.menu_list button"))
        )

        found_button = None
        # 2. 'ì¬ë¬´ì •ë³´' í…ìŠ¤íŠ¸ ê°€ì§„ ë²„íŠ¼ ì‹ë³„
        for button in buttons:
            if button.text.strip() == target_button_text:
                found_button = button
                break

        if found_button:
            # 3. ë²„íŠ¼ í´ë¦­
            found_button.click()
            print(f"ğŸ‘‰ ë²„íŠ¼'{target_button_text}' í´ë¦­.")

            # 4. í˜ì´ì§€ ë¡œë”©
            wait.until(EC.presence_of_element_located(
                (By.CLASS_NAME, "main_content"))
            )
            print(f"âœ… ë²„íŠ¼ í´ë¦­ìœ¼ë¡œ [ì¬ë¬´ì •ë³´] íƒ­ ì‚¬ì´íŠ¸ë¡œ ë¡œë”© ì™„ë£Œ.")

            return BeautifulSoup(driver.page_source, 'html.parser')
        else:
            print(f"âš ï¸ ì˜¤ë¥˜: '{target_button_text}' í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

    except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: '{target_button_text}' ë²„íŠ¼ì„ ì°¾ê±°ë‚˜ í´ë¦­í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒ - {e}")
            return None

# ì¬ë¬´í˜„í™© ë°ì´í„° ì¶”ì¶œ
def extract_financial_info(driver, company_data):
    """
    ì¬ë¬´í˜„í™©(ë§¤ì¶œ, ì˜ì—…ì´ìµ, ìˆœì´ìµ, ìë³¸ê¸ˆ)ì— ëŒ€í•œ ì¬ë¬´ì •ë³´ ë°ì´í„° ì¶”ì¶œ
    """
    print(f"=== extract_financial_info í•¨ìˆ˜ ì‹¤í–‰ ===")
    # ì¬ë¬´ì •ë³´ íƒ­ìœ¼ë¡œ ì´ë™
    financial_soup = get_financial_info_after_button(driver, "ì¬ë¬´ì •ë³´")

    # ì¬ë¬´í˜„í™© ë°ì´í„° div ì„ íƒ        
    financial_sections = financial_soup.find_all('div', class_='box_finance')

    for box_finance in financial_sections:
        # ì¬ë¬´ í•„ë“œëª… (ì˜ˆ: ë§¤ì¶œì•¡, ì˜ì—…ì´ìµ) ì¶”ì¶œ
        field_name_tag = box_finance.find('h3', class_='tit_finance')
        if not field_name_tag:
            continue
        field_name = field_name_tag.text.strip()
        print(f"ğŸ‘‰ field_name = {field_name}")

        # í•´ë‹¹ ì¬ë¬´ í•„ë“œì˜ ì—°ë„ë³„ ë°ì´í„° ì¶”ì¶œ
        area_graph = box_finance.find('div', class_='area_graph')
        if area_graph:
            wrap_graphs = area_graph.find_all('div', class_='wrap_graph')
            for graph in wrap_graphs:
                year = graph.find('em', class_='tit_graph').text.strip()
                value_str = graph.find('span', class_='txt_value').text.strip()

                # company_data["financial_history"] ë”•ì…”ë„ˆë¦¬ì— ë°ì´í„° ì €ì¥
                if year not in company_data["financial_history"]:
                    company_data["financial_history"][year] = {}
                
                # ìš”ì²­í•˜ì‹  í•„ë“œëª…ìœ¼ë¡œ ë§¤í•‘ (ì´ìì‚°, ìë³¸ ì´ê³„ëŠ” HTMLì— ì—†ì–´ í˜„ì¬ ì¶”ì¶œ ë¶ˆê°€)
                if field_name == "ë§¤ì¶œì•¡":
                    company_data["financial_history"][year]["ë§¤ì¶œì•¡"] = value_str
                elif field_name == "ì˜ì—…ì´ìµ":
                    company_data["financial_history"][year]["ì˜ì—…ì´ìµ"] = value_str
                elif field_name == "ë‹¹ê¸°ìˆœì´ìµ":
                    company_data["financial_history"][year]["ë‹¹ê¸°ìˆœì´ìµ"] = value_str
                elif field_name == "ìë³¸ê¸ˆ":
                    company_data["financial_history"][year]["ìë³¸ê¸ˆ"] = value_str

                print(f"âœ… {field_name} => ë…„ë„{year}:{value_str}")

CHROME_DRIVER_PATH = "C:\\Users\\okoko\\Downloads\\chromedriver-win64\\chromedriver.exe" # ë³¸ì¸ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”!
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36"

# === ì‚¬ëŒì¸ ì›¹í¬ë¡¤ë§ === 
def crawl_from_saramin(search_keyword: str) -> dict:
    """
    ì‚¬ëŒì¸ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì—… ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜
    """
    print(f"=== crawl_from_saramin í•¨ìˆ˜ ì‹¤í–‰ - '{search_keyword}' ê¸°ì—… ì •ë³´ ìˆ˜ì§‘ ì‹œì‘ ===")

    company_data = {
        "name": "", # íšŒì‚¬ëª…
        "established_year": "", # ì„¤ë¦½ ì—°ë„
        "company_type": "", # íšŒì‚¬ ìœ í˜• (ì£¼ì‹íšŒì‚¬, ìœ í•œíšŒì‚¬ ë“±)
        "is_listed": "", # ìƒì¥ ì—¬ë¶€
        "homepage": "", # íšŒì‚¬ í™ˆí˜ì´ì§€ URL
        "description": "", # íšŒì‚¬ ì„¤ëª…
        "address": "", # íšŒì‚¬ ì£¼ì†Œ
        "industry": "", # ì‚°ì—… ë¶„ì•¼ ì •ë³´
        "products_services": "", # ì œí’ˆ/ì„œë¹„ìŠ¤ ì´ë¦„ ë°°ì—´
        "key_executive": "", # ì£¼ìš” ê²½ì˜ì§„(ëŒ€í‘œì) ì´ë¦„
        "employee_count": "", # í˜„ì¬ ì§ì› ìˆ˜
        "employee_history": "", # ê³¼ê±° ì§ì› ìˆ˜ ì¶”ì´
        "latest_revenue": "", # ìµœê·¼ ë§¤ì¶œì•¡ (ì›)
        "latest_operating_income": "", # ìµœê·¼ ì˜ì—…ì´ìµ (ì›)
        "latest_net_income": "", # ìµœê·¼ ìˆœì´ìµ (ì›)
        "latest_fiscal_year": "", # ìµœê·¼ ì¬ë¬´ ì •ë³´ì˜ íšŒê³„ì—°ë„
        "financial_history": {}, # ê³¼ê±° ì¬ë¬´ ì •ë³´ íˆìŠ¤í† ë¦¬
        "total_funding": "", # ì´ íˆ¬ì ìœ ì¹˜ ê¸ˆì•¡ (ì›)
        "latest_funding_round": "", # ìµœê·¼ íˆ¬ì ë¼ìš´ë“œ ëª…ì¹­
        "latest_funding_date": "", # ìµœê·¼ íˆ¬ì ë‚ ì§œ
        "latest_valuation": "", # ìµœê·¼ ê¸°ì—…ê°€ì¹˜ (ì›)
        "investment_history": "", # íˆ¬ì íˆìŠ¤í† ë¦¬
        "investors": "", # ì£¼ìš” íˆ¬ìì ëª©ë¡
        "market_cap": "", # ì‹œê°€ì´ì•¡ (ì›)
        "stock_ticker": "", # ì£¼ì‹ ì¢…ëª© ì½”ë“œ
        "stock_exchange": "", # ìƒì¥ëœ ì¦ê¶Œê±°ë˜ì†Œ
        "patent_count": "", # íŠ¹í—ˆ ìˆ˜
        "trademark_count": "", # ìƒí‘œ ìˆ˜
        "ip_details": "", # ì§€ì‹ì¬ì‚° ìƒì„¸ ì •ë³´
        "tech_stack": "", # ê¸°ìˆ  ìŠ¤íƒ í‚¤ì›Œë“œ
        "recent_news": "" # ìµœê·¼ ë‰´ìŠ¤/ë³´ë„ìë£Œ
    }

    saramin_company_url = ""
    logo_url = ""

    driver = None
    try:
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument(f"user-agent={USER_AGENT}")
        chrome_options.add_argument("--log-level=3")
        chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])

        service = Service(executable_path=CHROME_DRIVER_PATH)
        driver = webdriver.Chrome(service=service, options=chrome_options)
        # print("âœ… Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì„±ê³µ.")
        
        SARAMIN_BASIC_URL = "https://www.saramin.co.kr"
        # ê²€ìƒ‰ì–´ URL ì¸ì½”ë”© ë° ê²€ìƒ‰ URL ìƒì„±
        encoded_search_keyword = requests.utils.quote(search_keyword)
        initial_search_url = f"{SARAMIN_BASIC_URL}/zf_user/search/company?search_area=main&search_done=y&search_optional_item=n&searchType=search&searchword={encoded_search_keyword}"
        
        # --- 1ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ë¡œ ì´ë™ ---
        print(f"ğŸ‘‰ ì‚¬ëŒì¸ '{encoded_search_keyword}' ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™: {initial_search_url}")
        driver.get(initial_search_url)

        # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        try:
            # `.cnt_result`ëŠ” ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ë¥¼, `.corp_name > a`ëŠ” ì²« ë²ˆì§¸ íšŒì‚¬ ë§í¬ë¥¼ ì˜ë¯¸
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, '.cnt_result, .corp_name > a'))
            )
            print("âœ… ì‚¬ëŒì¸ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ.")
        except Exception as e:
            print(f"âš ï¸ ì‚¬ëŒì¸ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨ ë˜ëŠ” ìš”ì†Œ ë¯¸ë°œê²¬: {e}")
            print(f"âŒ '{search_keyword}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return company_data # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì—¬ê¸°ì„œ í•¨ìˆ˜ ì¢…ë£Œ

        # ë¡œë“œëœ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ HTML íŒŒì‹±
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')

        # 1-1. ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ í™•ì¸ ë° ì²« ë²ˆì§¸ ê¸°ì—… ë§í¬ ì¶”ì¶œ
        cnt_result_span = soup.select_one('.cnt_result')
        result_count = 0
        if cnt_result_span:
            cnt_text = cnt_result_span.get_text(strip=True)
            match = re.search(r'\d+', cnt_text)
            if match:
                result_count = int(match.group(0))
        
        print(f"ì‚¬ëŒì¸ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {result_count}ê±´")

        # 1-2. result_count > 0 AND search_keyword == corp_name
        if result_count > 0:
            company_popup_names = soup.select('.company_popup')
            print(f">> company_popup_names (ì¡°íšŒëœ ê°œìˆ˜): {len(company_popup_names)}")

            # 1-3. ê²€ìƒ‰ ê¸°ì—…ëª…(search_keyword)ê³¼ ì—¬ëŸ¬ ê²€ìƒ‰ ê²°ê³¼ ì¤‘ ê¸°ì—…ëª…(corp_name)ì´ ì¼ì¹˜í•˜ëŠ” ê²ƒ ì„ íƒ
            found_match = False

            for corp_name_element in company_popup_names:
                corp_name = corp_name_element.attrs.get('title')

                if not corp_name:
                    continue

                corp_name = corp_name.strip()

                result = compare_company_name(search_keyword, corp_name)

                if result == True:
                    company_data["name"] = corp_name
                    company_name_link_suffix = corp_name_element.attrs.get('href')
                    if company_name_link_suffix:
                        saramin_company_url = requests.compat.urljoin(SARAMIN_BASIC_URL, company_name_link_suffix)
                    
                    print(f"âœ… ì‚¬ëŒì¸ì—ì„œ ì°¾ì€ ê¸°ì—…ëª…: '{company_data['name']}'")
                    print(f"âœ… ì‚¬ëŒì¸ ê¸°ì—… ìƒì„¸ ë§í¬: {saramin_company_url}")
                    
                    found_match = True                
                    break
            
            if not found_match:
                print(f"âŒ '{search_keyword}'ì— ì¼ì¹˜í•˜ëŠ” ê²€ìƒ‰ ê²°ê³¼ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return company_data
        else: # result_count == 0ì¸ ê²½ìš° 
            print(f"âŒ ê²€ìƒ‰ í‚¤ì›Œë“œ '{search_keyword}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ 0ê±´ì…ë‹ˆë‹¤.")
            return company_data

        # --- 2ë‹¨ê³„: ê¸°ì—… ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™í•˜ì—¬ ì •ë³´ ì¶”ì¶œ ---
        if saramin_company_url != "":
            print(f"ğŸ‘‰ ê¸°ì—… ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™: {saramin_company_url}")
            try:
                driver.get(saramin_company_url)
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, '.company_details')) # ìƒì„¸ ì •ë³´ ì»¨í…Œì´ë„ˆ
                )
                print("âœ… íšŒì‚¬ ìƒì„¸ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ.")
            except Exception as e:
                print(f"âš ï¸ íšŒì‚¬ ìƒì„¸ í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨ ë˜ëŠ” ìš”ì†Œ ë¯¸ë°œê²¬ (URL: {saramin_company_url}): {e}")
                print("í¬ë¡¤ë§ì„ ê³„ì† ì‹œë„í•˜ì§€ë§Œ, ì •ë³´ê°€ ë¶ˆì™„ì „í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                pass # ì‹¤íŒ¨ ì‹œì—ë„ ë‹¤ìŒ ì •ë³´ ì¶”ì¶œ ë¡œì§ ì§„í–‰

            # ìƒì„¸ í˜ì´ì§€ HTML íŒŒì‹±
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')

            # --- [ê¸°ì—…ì†Œê°œ] íƒ­ì—ì„œ ì •ë³´ ì¶”ì¶œ --- 
            # íšŒì‚¬ ìœ í˜•(ì£¼ì‹íšŒì‚¬, ìœ í•œíšŒì‚¬ ë“±)
            if "(ì£¼)" in corp_name or "(ì£¼ì‹íšŒì‚¬)" in corp_name:
                company_data["company_type"] = "ì£¼ì‹íšŒì‚¬"
                company_data["is_listed"] = True
            elif "(ìœ )" in corp_name or "(ìœ í•œíšŒì‚¬)" in corp_name:
                company_data["company_type"] = "ìœ í•œíšŒì‚¬" 
            else:
                company_data["is_listed"] = False

            # ì§ì›ìˆ˜
            employee_count_element = soup.select_one('.company_summary_item:nth-child(3) .box_align')
            if employee_count_element:
                company_data["employee_count"] = employee_count_element.select_one('.company_summary_tit').text.strip()

            # ì‚°ì—… ë¶„ì•¼
            industry_element = soup.select_one('.company_details_group:nth-child(1) dd')
            if industry_element:
                company_data["industry"] = industry_element.text.strip()
            
            # ì œí’ˆ/ì„œë¹„ìŠ¤ ëª…(JSON)
            products_services_element = soup.select_one('dt:-soup-contains("ë¸Œëœë“œëª…") + dd > p')
            if products_services_element:
                company_data["products_services"] = products_services_element.text.strip()

            # CEO
            key_executives_element = soup.select_one('.company_details_group:nth-child(2) dd')
            if key_executives_element:
                company_data['key_executive'] = key_executives_element.text.strip()

            # í™ˆí˜ì´ì§€ URL
            homepage_element = soup.select_one('dt:-soup-contains("í™ˆí˜ì´ì§€") + dd > a')
            if homepage_element:
                company_data["homepage"] = homepage_element.get('href')
            
            # íšŒì‚¬ ì£¼ì†Œ
            address_element = soup.select_one('dt:-soup-contains("ì£¼ì†Œ") + dd > p')
            if address_element:
                company_data["address"] = address_element.text.strip() # ì£¼ì†Œë§Œ ì¶œë ¥(ì§€ë„ ì œì™¸)

            # ì„¤ë¦½ì¼
            founded_element = soup.select_one('.company_summary_item:nth-child(1) .company_summary_desc')
            if founded_element:
                raw_date = founded_element.text.strip() # yyyy-mm-dd í˜•ì‹

                company_data["established_year"] = raw_date
                print(f"ì„¤ë¦½ì¼ = {company_data["established_year"]}")

            # íšŒì‚¬ ìš”ì•½/ì„¤ëª…
            summary_element = soup.select_one('.company_introduce .txt')
            if summary_element:
                company_data["description"] = summary_element.text.strip()

            # ë¡œê³  URL
            logo_element = soup.select_one('.box_logo img')
            if logo_element and 'src' in logo_element.attrs:
                logo_url = logo_element['src']
                print(f"logo_url = {logo_url}")

            # --- 3ë‹¨ê³„: ê¸°ì—… ìƒì„¸ í˜ì´ì§€ì˜ [ì¬ë¬´ì •ë³´] íƒ­ì—ì„œ ì¬ë¬´í˜„í™© financial_history ì¶”ì¶œ ---
            extract_financial_info(driver, company_data)

            # "financial_history" ì—ì„œ ìµœê·¼ ë§¤ì¶œì•¡,ì˜ì—…ì´ìµ,ë‹¹ê¸°ìˆœì´ìµ,íšŒê³„ì—°ë„ ì¶œë ¥
            if company_data["financial_history"]:
                latest_year = max(company_data["financial_history"].keys(), key=int)

                latest_financial_data = company_data["financial_history"][latest_year]

                company_data["latest_fiscal_year"] = latest_year
                company_data["latest_revenue"] = latest_financial_data.get("ë§¤ì¶œì•¡")
                company_data["latest_operating_income"] = latest_financial_data.get("ì˜ì—…ì´ìµ")
                company_data["latest_net_income"] = latest_financial_data.get("ë‹¹ê¸°ìˆœì´ìµ")

                print(f"ìµœê·¼ íšŒê³„ì—°ë„: {company_data['latest_fiscal_year']}")
                print(f"ìµœê·¼ ë§¤ì¶œì•¡: {company_data['latest_revenue']}")
                print(f"ìµœê·¼ ì˜ì—…ì´ìµ: {company_data['latest_operating_income']}")
                print(f"ìµœê·¼ ìˆœì´ìµ: {company_data['latest_net_income']}")
            else:
                print("ğŸš« financial_historyê°€ ë¹„ì–´ ìˆì–´ ìµœì‹  ì¬ë¬´ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"ğŸš« í¬ë¡¤ë§ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        pass

    finally:
        if driver:
            driver.quit()
            print("âœ… Chrome ë“œë¼ì´ë²„ ì¢…ë£Œ.")
    
    return company_data


if __name__ == "__main__":
    test_keyword = input("ê¸°ì—…ëª…: ")
    print(f"\n===== ê¸°ì—… í¬ë¡¤ë§ ì‹œì‘: '{test_keyword}' =====")

    # 1. ê¸°ì—… ì •ë³´ í¬ë¡¤ë§
    crawled_data = crawl_from_saramin(test_keyword)

    print("\n--- í¬ë¡¤ë§ ê²°ê³¼ ---")
    print(json.dumps(crawled_data, indent=2, ensure_ascii=False))
    print("==================================\n")
