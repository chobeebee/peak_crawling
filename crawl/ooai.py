import json
import re
import urllib.parse
import requests
import time
from collections import Counter # ë¹ˆë„ìˆ˜ ê³„ì‚°ì„ ìœ„í•´ ì¶”ê°€ 
from config.setting import USER_AGENT 

def ooai_crawler(query: str) -> dict:
    """
    oo.aiì— íŠ¹ì • ì¿¼ë¦¬ ê²€ìƒ‰í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ íŒŒì‹±í•˜ì—¬ ì „í™˜

    Args:
    query(str): ê²€ìƒ‰í•  ê²€ìƒ‰ì–´ (ì˜ˆ:  "ì‚¼ì„±ì „ì ì£¼ìš” íƒ€ê²Ÿ ê³ ê°ì¸µ") 
  
    Returns:
    dict: ê²€ìƒ‰ ê²°ê³¼ì™€ ê´€ë ¨ëœ ì •ë³´ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
        {
            'json': {'search_id': ..., 'full_html_answer': ..., 'plain_text_answer': ...}}
        }
        ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜.
    """  
    encoded_query = urllib.parse.quote(query)

    # 1. CSRF í† í° ì¶”ì¶œ
    url = f'https://oo.ai/search?q={encoded_query}'
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"[{query}] ì´ˆê¸° í˜ì´ì§€ ì ‘ê·¼ ì˜¤ë¥˜: {e}")
        return {}

    html = response.text
    token = None
    # ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ token ê°’ ì¶”ì¶œ
    token_match = re.search(r'token:\s*"([^"]+)"', html)
    if token_match:
        token = token_match.group(1)

    # ì¶”ì¶œëœ í† í°ì´ ì—†ë‹¤ë©´ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
    if not token:
        print(f"[{query}] CSRF í† í°ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {}
    
    # ì¶”ì¶œëœ í† í°ì„ ì¶œë ¥
    print({"json": {"csrf_token": token}})

    # 2. ê²€ìƒ‰ API í˜¸ì¶œ
    search_url = f"https://oo.ai/api/search?q={encoded_query}&lang=ko&tz=Asia/Seoul"
    headers = {
        "accept": "*/*",
        "accept-language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        "cookie": "_variant=stable; lang=ko",
        "origin": "https://oo.ai",
        "referer": f"https://oo.ai/search?q={encoded_query}",
        "user-agent": USER_AGENT,
        "x-csrf-token": f"Bearer {token}"
    }

    try:
        response = requests.post(search_url, headers=headers, timeout=20)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"[{query}] ê²€ìƒ‰ API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return {}
    
    parsed_data = parse_sse_response(response.text)
    print(json.dumps(parsed_data, ensure_ascii=False, indent=4))

    return parsed_data

def parse_sse_response(stream_data:str) -> dict:
    """
    SSE í˜•ì‹ì˜ ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ìµœì¢… ë‹µë³€ ì¶”ì¶œ

    Args:
        stream_data(str) : SEE í˜•ì‹ì˜ ë¬¸ìì—´ ë°ì´í„°
    
    Returns:
        dict: íŒŒì‹±ëœ ê²€ìƒ‰ ê²°ê³¼(search_id, full_html_answer, plain_text_answer)
    """
    lines = stream_data.split('\n')
    final_answer_html = None
    sid = None
    for line in lines:
        if line.startswith('data:'):
            try:
                json_data_string = line[5:].strip()
                if json_data_string:
                    parsed_data = json.loads(json_data_string)
                    if parsed_data.get('type') == 'save':
                        if 'answer' in parsed_data:
                            final_answer_html = parsed_data['answer']
                        if 'id' in parsed_data:
                            sid = parsed_data['id']
                        if final_answer_html and sid:
                            break
            except Exception:
                # JSON íŒŒì‹± ì˜¤ë¥˜ëŠ” ë¬´ì‹œí•˜ê³  ë‹¤ìŒ ë¼ì¸ìœ¼ë¡œ ì§„í–‰
                continue

    plain_text_answer = None
    if final_answer_html:
        # <webblock> íƒœê·¸ì™€ ê·¸ ë‚´ìš© ì œê±°
        temp_answer = re.sub(r'<webblock>.*?</webblock>', '', final_answer_html, flags=re.DOTALL).strip()
        # ë‚˜ë¨¸ì§€ HTML íƒœê·¸ë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜ í›„, ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê³  ì•ë’¤ ê³µë°± ì œê±°
        plain_text_answer = re.sub(r'<[^>]+>', ' ', temp_answer)
        plain_text_answer = re.sub(r'\s+', ' ', plain_text_answer).strip()
    return {
        'json': {
            'search_id': sid,
            'full_html_answer': final_answer_html,
            'plain_text_answer': plain_text_answer
        }
    }

def enrich_company_data(company_name: str, existing_data: dict) -> dict:
    """
    ê¸°ì¡´ ê¸°ì—… ë°ì´í„°ì—ì„œ ë¶€ì¡±í•œ í•„ë“œë¥¼ oo.ai ê²€ìƒ‰ì„ í†µí•´ ë³´ì™„
    Args:
        company_name(str): ê¸°ì—…ëª…
        existing_data(dict): í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ê¸°ì—… ë°ì´í„° (í•„ë“œ: ê°’)
    
    Returns:
        dict: oo.ai ê²€ìƒ‰ì„ í†µí•´ ë³´ì™„ëœ ê¸°ì—… ë°ì´í„°(ì¶”í›„ í†µí•© ë°ì´í„°ì™€ ë³‘í•© ì˜ˆì •)   
    """
    # ë³´ì™„í•  í•„ë“œ
    fields_to_enrich = {
        "target_customers": f"{company_name} ê¸°ì—…ì˜ ì£¼ìš” ëª©í‘œ ê³ ê°ì¸µ",
        "competitors": f"{company_name} ê¸°ì—…ì˜ ì£¼ìš” ê²½ìŸì‚¬",
        "strengths": f"{company_name} ê¸°ì—…ì˜ ê°•ì ",
        "risk_factors": f"{company_name} ê¸°ì—…ì˜ ìœ„í—˜ ìš”ì¸",
        "recent_trends": f"{company_name} ê¸°ì—…ì˜ ìµœê·¼ ë™í–¥"
    }
    
    extra_prompt_guide = "ì— ëŒ€í•´ ë‹¤ìŒ ê°€ì´ë“œë¼ì¸ì„ ì—„ìˆ˜í•˜ì—¬ 1ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ë§Œ ìš”ì•½í•´ ì£¼ì„¸ìš”: 1. ë¶ˆí•„ìš”í•œ ì„œë¡ /ê²°ë¡  ì—†ì´ ë°”ë¡œ ë³¸ë¡ ë¶€í„° ì‹œì‘. 2. ê°ê´€ì ì¸ ì •ë³´ë§Œ í¬í•¨. 3. ê°€ëŠ¥í•œ í•œ ìˆ˜ì¹˜ë‚˜ ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œ ì„œìˆ . 4. ~ì´ë‹¤/ì…ë‹ˆë‹¤ ì²´ ì¢…ê²° 5. ê´€ë ¨ ì •ë³´ê°€ ì—†ì„ ê²½ìš° í…ìŠ¤íŠ¸ ëŒ€ì‹  ''ìœ¼ë¡œ ì¶œë ¥."
    
    # ë™ì¼ ì¿¼ë¦¬ ë°˜ë³µ ì‹¤í–‰ íšŸìˆ˜
    NUM_RETRIES = 3 # ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œí•˜ì—¬ ì¼ê´€ì„± í™•ë³´

    final_data = existing_data.copy()

    for field, query_template in fields_to_enrich.items():
        # í˜„ì¬ í•„ë“œ ê°’ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        if not final_data.get(field):
            print(f"ğŸ”'{field}' í•„ë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. OO.aiì—ì„œ ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤: '{query_template}'")
            
            collected_answers = []
            for i in range(NUM_RETRIES):
                print(f"  > ì‹œë„ {i+1}/{NUM_RETRIES}...")
                search_result = ooai_crawler(query_template + extra_prompt_guide)
                if search_result and search_result['json'].get('plain_text_answer'):
                    answer = search_result['json']['plain_text_answer'].strip()
                    if answer: # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ìœ íš¨í•œ ë‹µë³€ë§Œ ì¶”ê°€
                        collected_answers.append(answer)
                # ì•½ê°„ì˜ ë”œë ˆì´ë¥¼ ì£¼ì–´ API í˜¸ì¶œ ê°„ê²©ì„ ë„ì›€
                time.sleep(3)

            if collected_answers:
                # ê°€ì¥ ë¹ˆë²ˆí•˜ê²Œ ë‚˜ì˜¨ ë‹µë³€ ì„ íƒ (ë‹¤ì–‘í•œ ë‹µë³€ì´ ë‚˜ì˜¬ ê²½ìš° ì²« ë²ˆì§¸ ì„ íƒ)
                # Counterë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë‹µë³€ì˜ ë¹ˆë„ìˆ˜ë¥¼ ì„¸ê³ , ê°€ì¥ ë§ì€ ë¹ˆë„ìˆ˜ë¥¼ ê°€ì§„ ë‹µë³€ì„ ì„ íƒ
                most_common_answer = Counter(collected_answers).most_common(1)
                if most_common_answer:
                    chosen_answer = most_common_answer[0][0]
                    final_data[field] = chosen_answer
                    print(f"âœ…'{field}' í•„ë“œ ì±„ì›€ (ìµœë‹¤ë¹ˆë„): {chosen_answer[:50]}...")
                else: # Counterê°€ ë¹„ì–´ìˆë‹¤ë©´ (ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ì§€ë§Œ ë°©ì–´ ì½”ë“œ)
                    print(f"âŒ '{field}' í•„ë“œì— ëŒ€í•œ OO.ai ê²€ìƒ‰ ê²°ê³¼ê°€ ì¼ê´€ë˜ì§€ ì•Šê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    final_data[field] = ""
            else:
                print(f"âŒ '{field}' í•„ë“œì— ëŒ€í•œ OO.ai ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                final_data[field] = ""

    return final_data

if __name__ == "__main__":
    company_name = input("ê¸°ì—…ëª…: ")
    print(f"\n===== oo.ai ì •ë³´ ìˆ˜ì§‘ ì‹œì‘: '{company_name}' =====")

    # 1. ì´ˆê¸° í¬ë¡¤ë§ ë°ì´í„° (ì˜ˆì‹œ: ì¼ë¶€ í•„ë“œê°€ ë¹„ì–´ìˆìŒ)
    # initial_company_data = crawl_from_saramin(company_name)
    initial_company_data = {
        "name": "(ì£¼)ê¹¨ë—í•œ",
        "established_year": "2013ë…„ 2ì›” 5ì¼ ì„¤ë¦½",
        "company_type": "ì£¼ì‹íšŒì‚¬",
        "is_listed": True,
        "homepage": "",
        "description": "",
        "address": "ê²½ê¸° ê´‘ì£¼ì‹œ ê´‘ì£¼ëŒ€ë¡œ105ë²ˆê¸¸ 5-9",
        "industry": "í† ëª©ì‹œì„¤ë¬¼ ê±´ì„¤ì—…",
        "products_services": "",
        "key_executive": "ê¹€ì¢…ì°¬",
        "employee_count": "",
        "employee_history": "",
        "latest_revenue": "",
        "latest_operating_income": "- 6,561ë§Œì›",
        "latest_net_income": "- 2ì–µ 9,627ë§Œì›",
        "latest_fiscal_year": "2024",
        "financial_history": {
            "2021": {
                "ë§¤ì¶œì•¡": "2,764ë§Œì›",
                "ì˜ì—…ì´ìµ": "- 4,096ë§Œì›",
                "ë‹¹ê¸°ìˆœì´ìµ": "5ì–µ 364ë§Œì›",
                "ìë³¸ê¸ˆ": "5,000ë§Œì›"
            },
            "2022": {
                "ë§¤ì¶œì•¡": "923ë§Œì›",
                "ì˜ì—…ì´ìµ": "- 1ì–µ 824ë§Œì›",
                "ë‹¹ê¸°ìˆœì´ìµ": "10ì–µ 4,653ë§Œì›",
                "ìë³¸ê¸ˆ": "5,000ë§Œì›"
            },
            "2023": {
                "ì˜ì—…ì´ìµ": "- 1ì–µ 8,023ë§Œì›",
                "ë‹¹ê¸°ìˆœì´ìµ": "- 3ì–µ 8,809ë§Œì›",
                "ìë³¸ê¸ˆ": "5,000ë§Œì›"
            },
            "2024": {
                "ì˜ì—…ì´ìµ": "- 6,561ë§Œì›",
                "ë‹¹ê¸°ìˆœì´ìµ": "- 2ì–µ 9,627ë§Œì›",
                "ìë³¸ê¸ˆ": "5,000ë§Œì›"
            }
        },
        "total_funding": "", # ì´ íˆ¬ì ìœ ì¹˜ ê¸ˆì•¡ (ì›)
        "latest_funding_round": "", # ìµœê·¼ íˆ¬ì ë¼ìš´ë“œ ëª…ì¹­
        "latest_funding_date": "", # ìµœê·¼ íˆ¬ì ë‚ ì§œ
        "latest_valuation": "", # ìµœê·¼ ê¸°ì—…ê°€ì¹˜ (ì›)
        "investment_history": "", # íˆ¬ì íˆìŠ¤í† ë¦¬
        "investors": "", # ì£¼ìš” íˆ¬ìì ëª©ë¡
        "market_cap": "", # ì‹œê°€ì´ì•¡ (ì›)
        "stock_ticker": "", # ì£¼ì‹ ì¢…ëª© ì½”ë“œ
        "stock_exchange": "", # ìƒì¥ëœ ì¦ê¶Œê±°ë˜ì†Œ
        "patent_count": "", # íŠ¹í—ˆ ìˆ˜
        "trademark_count": "", # ìƒí‘œ ìˆ˜
        "ip_details": "", # ì§€ì‹ì¬ì‚° ìƒì„¸ ì •ë³´
        "tech_stack": "", # ê¸°ìˆ  ìŠ¤íƒ í‚¤ì›Œë“œ
        "recent_news": "", # ìµœê·¼ ë‰´ìŠ¤/ë³´ë„ìë£Œ
        "target_customers": "",
        "competitors": "", 
        "strengths": "",
        "risk_factors": "",  
        "recent_trends": ""
    }
    print("\n--- 'ì‚¬ëŒì¸' ì´ˆê¸° ìˆ˜ì§‘ ë°ì´í„° (ì¼ë¶€ í•„ë“œ ëˆ„ë½) ---")
    print(json.dumps(initial_company_data, ensure_ascii=False, indent=4))

    # 2. oo.ai í™œìš©í•˜ì—¬ ë¶€ì¡±í•œ ê¸°ì—… ì •ë³´ í•„ë“œ ë³´ì™„
    print("\n--- OO.aië¥¼ í™œìš©í•œ ë°ì´í„° ë³´ì™„ ì‹œì‘ ---")
    final_company_data = enrich_company_data(initial_company_data.get('name'), initial_company_data)
    # crawled_data = ooai_crawler(company_name)

    print("\n--- ìµœì¢… í†µí•© ë°ì´í„° ---")
    print(json.dumps(final_company_data, ensure_ascii=False, indent=4))